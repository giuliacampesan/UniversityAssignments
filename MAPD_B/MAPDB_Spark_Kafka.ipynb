{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In the terminal__:\n",
    "\n",
    "connect ssh to _kafka_ (IP:10.67.22.185)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ssh kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and start zookeeper and kafka servers and exit connection:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kafka_2.13-2.7.0/bin/zookeeper-server-start.sh kafka_2.13-2.7.0/config/zookeeper.properties &\n",
    "\n",
    "kafka_2.13-2.7.0/bin/kafka-server-start.sh kafka_2.13-2.7.0/config/server.properties &\n",
    "\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages needed and __broker__ IP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'kafka:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='stream', error_code=0), (topic='results', error_code=0)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.delete_topics(['stream', 'results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__consumer_offsets']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='stream', error_code=0, error_message=None), (topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_topic = NewTopic(name='stream', \n",
    "                      num_partitions=1, \n",
    "                      replication_factor=1)\n",
    "\n",
    "results_topic = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "kafka_admin.create_topics(new_topics=[stream_topic, results_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME=/usr/local/spark/\r\n",
      "PYSPARK_PYTHON=/usr/bin/python3.6\r\n",
      "PATH=/usr/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/spark/bin:/root/bin\r\n"
     ]
    }
   ],
   "source": [
    "!env | grep -i spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark//logs/spark-root-org.apache.spark.deploy.master.Master-1-mapd-b-gr12-1.novalocal.out\n",
      "master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr12-1.novalocal.out\n",
      "slave01: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr12-2.novalocal.out\n",
      "slave03: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr12-5.novalocal.out\n",
      "slave02: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr12-3.novalocal.out\n"
     ]
    }
   ],
   "source": [
    "! $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"Project_MAPDB_application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_MAPDB_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://master:7077 appName=Project_MAPDB_application>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'stream')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, LongType\n",
    "\n",
    "## the schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\", IntegerType()),\n",
    "                StructField(\"FPGA\", StringType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\", StringType()),\n",
    "                StructField(\"BX_COUNTER\", StringType()),\n",
    "                StructField(\"TDC_MEAS\", StringType())    \n",
    "        ]\n",
    ")\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: string (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: string (nullable = true)\n",
      " |    |-- BX_COUNTER: string (nullable = true)\n",
      " |    |-- TDC_MEAS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: string (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: string (nullable = true)\n",
      " |-- BX_COUNTER: string (nullable = true)\n",
      " |-- TDC_MEAS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat, col, lit, count, desc, asc , to_json\n",
    "from itertools import chain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(df, epoch_id):   \n",
    "    \n",
    "    #total events\n",
    "    tot = df.count()\n",
    "    \n",
    "    #clean\n",
    "    df_clean = df.where(col('HEAD')== 2)\n",
    "    \n",
    "    #point 1\n",
    "    tot_hits = df_clean.count()\n",
    "\n",
    "    #chambers\n",
    "    chamber_0= df_clean \\\n",
    "        .where(col('FPGA') == 0)\\\n",
    "        .where(col('TDC_CHANNEL') >= 0)\\\n",
    "        .where(col('TDC_CHANNEL') <= 63)\n",
    "\n",
    "    chamber_1= df_clean\\\n",
    "        .where(col('FPGA') == 0)\\\n",
    "        .where(col('TDC_CHANNEL') >= 64)\\\n",
    "        .where(col('TDC_CHANNEL') <= 127)\n",
    "\n",
    "    chamber_2= df_clean\\\n",
    "        .where(col('FPGA') == 1)\\\n",
    "        .where(col('TDC_CHANNEL') >= 0)\\\n",
    "        .where(col('TDC_CHANNEL') <= 63)\\\n",
    "\n",
    "    chamber_3=df_clean\\\n",
    "        .where(col('FPGA') == 1)\\\n",
    "        .where(col('TDC_CHANNEL') >= 64)\\\n",
    "        .where(col('TDC_CHANNEL') <= 127)\\\n",
    "    \n",
    "    #point2\n",
    "    tot_hits_ch0 = chamber_0.count()\n",
    "    tot_hits_ch1 = chamber_1.count()\n",
    "    tot_hits_ch2 = chamber_2.count()\n",
    "    tot_hits_ch3 = chamber_3.count()\n",
    "    \n",
    "    #point 3\n",
    "    df0 = chamber_0.groupBy('TDC_CHANNEL').count().toPandas()\n",
    "    df1 = chamber_1.groupBy('TDC_CHANNEL').count().toPandas()\n",
    "    df2 = chamber_2.groupBy('TDC_CHANNEL').count().toPandas()\n",
    "    df3 = chamber_3.groupBy('TDC_CHANNEL').count().toPandas()\n",
    "    \n",
    "    #point 4    \n",
    "    \n",
    "    df_orbs0=chamber_0.groupBy('ORBIT_CNT').agg(F.countDistinct(\"TDC_CHANNEL\"))\\\n",
    "                .groupBy(col('count(TDC_CHANNEL)')).count().toPandas()\n",
    "    df_orbs1=chamber_1.groupBy('ORBIT_CNT').agg(F.countDistinct(\"TDC_CHANNEL\"))\\\n",
    "                .groupBy(col('count(TDC_CHANNEL)')).count().toPandas()\n",
    "    df_orbs2=chamber_2.groupBy('ORBIT_CNT').agg(F.countDistinct(\"TDC_CHANNEL\"))\\\n",
    "                    .groupBy(col('count(TDC_CHANNEL)')).count().toPandas()\n",
    "    df_orbs3=chamber_3.groupBy('ORBIT_CNT').agg(F.countDistinct(\"TDC_CHANNEL\"))\\\n",
    "            .groupBy(col('count(TDC_CHANNEL)')).count().toPandas()\n",
    "\n",
    "    #point Extra 1\n",
    "    tdc128 = df_clean.where(col('FPGA')==1).where(col('TDC_CHANNEL') ==128).toPandas()\n",
    "    l_orbs = tdc128['ORBIT_CNT'].tolist()\n",
    "    \n",
    "    scint_df_ch0 = chamber_0.where(col('ORBIT_CNT').isin(l_orbs)).groupBy('TDC_CHANNEL')\\\n",
    "                .count().toPandas()\n",
    "    scint_df_ch1 = chamber_1.where(col('ORBIT_CNT').isin(l_orbs)).groupBy('TDC_CHANNEL')\\\n",
    "                .count().toPandas()\n",
    "    scint_df_ch2 = chamber_2.where(col('ORBIT_CNT').isin(l_orbs)).groupBy('TDC_CHANNEL')\\\n",
    "                .count().toPandas()\n",
    "    scint_df_ch3 = chamber_3.where(col('ORBIT_CNT').isin(l_orbs)).groupBy('TDC_CHANNEL')\\\n",
    "                .count().toPandas()\n",
    "\n",
    "    outputJson = {'tot_import':tot,\n",
    "                  'hits': tot_hits,\n",
    "                  'hitsPerChamber': [tot_hits_ch0, tot_hits_ch1, tot_hits_ch2, tot_hits_ch3],\n",
    "                  'histo_ch0': [df0['TDC_CHANNEL'].tolist(), df0['count'].tolist()],\n",
    "                  'histo_ch1': [df1['TDC_CHANNEL'].tolist(), df1['count'].tolist()],\n",
    "                  'histo_ch2': [df2['TDC_CHANNEL'].tolist(), df2['count'].tolist()],\n",
    "                  'histo_ch3': [df3['TDC_CHANNEL'].tolist(), df3['count'].tolist()],\n",
    "                  'histo_orbit_ch0':[df_orbs0['count(TDC_CHANNEL)'].tolist(), df_orbs0['count'].tolist()],\n",
    "                  'histo_orbit_ch1':[df_orbs1['count(TDC_CHANNEL)'].tolist(), df_orbs1['count'].tolist()],\n",
    "                  'histo_orbit_ch2':[df_orbs2['count(TDC_CHANNEL)'].tolist(), df_orbs2['count'].tolist()],\n",
    "                  'histo_orbit_ch3':[df_orbs3['count(TDC_CHANNEL)'].tolist(), df_orbs3['count'].tolist()],\n",
    "                  'histo_scin_ch0': [scint_df_ch0['TDC_CHANNEL'].tolist(), scint_df_ch0['count'].tolist()],\n",
    "                  'histo_scin_ch1': [scint_df_ch1['TDC_CHANNEL'].tolist(), scint_df_ch1['count'].tolist()],\n",
    "                  'histo_scin_ch2': [scint_df_ch2['TDC_CHANNEL'].tolist(), scint_df_ch2['count'].tolist()],\n",
    "                  'histo_scin_ch3': [scint_df_ch3['TDC_CHANNEL'].tolist(), scint_df_ch3['count'].tolist()],\n",
    "                 }\n",
    "    \n",
    "    producer.send('results', json.dumps(outputJson).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF.writeStream\\\n",
    "    .foreachBatch(analysis)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: stopping org.apache.spark.deploy.worker.Worker\n",
      "slave01: stopping org.apache.spark.deploy.worker.Worker\n",
      "slave02: stopping org.apache.spark.deploy.worker.Worker\n",
      "slave03: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "! $SPARK_HOME/sbin/stop-all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop kafka and zookeeper servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh kafka kafka_2.13-2.7.0/bin/kafka-server-stop.sh \n",
    "! ssh kafka kafka_2.13-2.7.0/bin/zookeeper-server-stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
